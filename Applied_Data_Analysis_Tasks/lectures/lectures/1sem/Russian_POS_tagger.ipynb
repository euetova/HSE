{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфологические процессоры для русского языка\n",
    "\n",
    "Существует два основных вида морфологических процессоров: морфологические анализаторы и морфологические словари.\n",
    "Морфологический анализатор определяет часть речи слова по его парадигме. Умеет определять вероятность того или иного разбора. \n",
    "Морфологический словарь определяет часть речи на основе разметки словаря / корпуса. Может возвращать несколько разборов каждого слова, причем для каждого разбора будет определена его частота в размеченном корпусе. \n",
    "* mystem – морфологический анализатор Yandex (https://tech.yandex.ru/mystem/) . Утверждается, что 3-я версия mystem снимает омонимию.\n",
    "    *  pymystem3 –  обертка для Python (https://github.com/Digsolab/pymystem3)\n",
    "* pymorphy2 – морфологический словарь на основе словарей Открытого корпуса (https://pymorphy2.readthedocs.org/en/latest/)\n",
    "\n",
    "Морфологические процессоры чаще всего используются:\n",
    "1. для лемматизации (lemmatization)\n",
    "2. для определения части речи (Part-of-speech tagging, POS tagging)\n",
    "\n",
    "Эти задачи, разумеется, связанные, но вызовы морфологических процессоров различаются.\n",
    "\n",
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "красивый женщина красиво мыть рама и ножницы резать занавеска\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "text = u\"Красивые женщины красиво мыли раму и ножницами резали занавески\"\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "красивый\n",
      "женщина\n",
      "красиво\n",
      "мыть\n",
      "рам\n",
      "и\n",
      "ножницы\n",
      "резать\n",
      "занавеска\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "#возьмем 0 разбор – самый частый в ОК \n",
    "\n",
    "for word in text.split():\n",
    "    print morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word=u'\\u0440\\u0430\\u043c\\u0443', tag=OpencorporaTag('NOUN,inan,masc,Geox sing,datv'), normal_form=u'\\u0440\\u0430\\u043c', score=0.5, methods_stack=((<DictionaryAnalyzer>, u'\\u0440\\u0430\\u043c\\u0443', 32, 2),))\n",
      "Parse(word=u'\\u0440\\u0430\\u043c\\u0443', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form=u'\\u0440\\u0430\\u043c\\u0430', score=0.5, methods_stack=((<DictionaryAnalyzer>, u'\\u0440\\u0430\\u043c\\u0443', 55, 3),))\n"
     ]
    }
   ],
   "source": [
    "for i in morph.parse(u'раму'):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "красивый\n",
      "мама\n",
      "красиво\n",
      "мыло\n",
      "рам\n",
      "рам NOUN,inan,masc,Geox sing,datv\n",
      "рама NOUN,inan,femn sing,accs\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "#возьмем 0 разбор – самый частый в ОК \n",
    "\n",
    "for word in text.split():\n",
    "    print morph.parse(word)[0].normal_form\n",
    "\n",
    "\n",
    "# print morph.parse(u'стекло')\n",
    "# print len(morph.parse(u'стекло'))\n",
    "for i in morph.parse(u'раму'):\n",
    "    print i.normal_form, i.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как обстоят дела с омонимией. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "он видеть их семья свой глаз\n",
      "\n",
      "***\n",
      "он\n",
      "видеть\n",
      "они\n",
      "семь\n",
      "свой\n",
      "глаз\n"
     ]
    }
   ],
   "source": [
    "text = u\"Он видел их семью своими глазами\"\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))\n",
    "print '***'\n",
    "for word in text.split():\n",
    "    print morph.parse(word)[0].normal_form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full info: [{\"text\": \"Красивая\", \"analysis\": [{\"lex\": \"красивый\", \"gr\": \"A=им,ед,полн,жен\"}]}, {\"text\": \" \"}, {\"text\": \"мама\", \"analysis\": [{\"lex\": \"мама\", \"gr\": \"S,жен,од=им,ед\"}]}, {\"text\": \" \"}, {\"text\": \"красиво\", \"analysis\": [{\"lex\": \"красиво\", \"gr\": \"ADV=\"}]}, {\"text\": \" \"}, {\"text\": \"мыла\", \"analysis\": [{\"lex\": \"мыть\", \"gr\": \"V,несов,пе=прош,ед,изъяв,жен\"}]}, {\"text\": \" \"}, {\"text\": \"раму\", \"analysis\": [{\"lex\": \"рама\", \"gr\": \"S,жен,неод=вин,ед\"}]}, {\"text\": \"\\n\"}]\n",
      "Красивая\n",
      "Gram info: A=им,ед,полн,жен\n",
      "POS tag: A\n",
      "мама\n",
      "Gram info: S,жен,од=им,ед\n",
      "POS tag: S\n",
      "красиво\n",
      "Gram info: ADV=\n",
      "POS tag: ADV\n",
      "мыла\n",
      "Gram info: V,несов,пе=прош,ед,изъяв,жен\n",
      "POS tag: V\n",
      "раму\n",
      "Gram info: S,жен,неод=вин,ед\n",
      "POS tag: S\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "text = u\"Красивая мама красиво мыла раму\"\n",
    "gram_info = m.analyze(text)\n",
    "print \"full info:\", json.dumps(gram_info, ensure_ascii=False, encoding='utf8')\n",
    "for i in range(len(gram_info)):\n",
    "    if not gram_info[i][\"text\"].isspace():\n",
    "        print gram_info[i][\"text\"]\n",
    "        print 'Gram info:', gram_info[i][\"analysis\"][0][\"gr\"]\n",
    "        print 'POS tag:', gram_info[i][\"analysis\"][0][\"gr\"].split(',')[0].split('=')[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Красивая\n",
      "Gram info: ADJF,Qual femn,sing,nomn\n",
      "POS tag: ADJF\n",
      "мама\n",
      "Gram info: NOUN,anim,femn sing,nomn\n",
      "POS tag: NOUN\n",
      "красиво\n",
      "Gram info: ADVB\n",
      "POS tag: ADVB\n",
      "мыла\n",
      "Gram info: NOUN,inan,neut sing,gent\n",
      "POS tag: NOUN\n",
      "раму\n",
      "Gram info: NOUN,inan,masc,Geox sing,datv\n",
      "POS tag: NOUN\n"
     ]
    }
   ],
   "source": [
    "for word in text.split():\n",
    "    print word\n",
    "    print 'Gram info:', morph.parse(word)[0].tag\n",
    "    print 'POS tag:', morph.parse(word)[0].tag.POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синтаксис pymystem3 существенно сложнее, POS тэги не хранятся в явном виде. Для того, что бы узнать POS тэг приходится использовать чудовищную строчку gram_info[i][\"analysis\"][0][\"gr\"].split(',')[0].split('=')[0] . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Привет, как тебя зовут? Оля\n",
      "Оля, где ты была всю прошлую ночь?\n"
     ]
    }
   ],
   "source": [
    "verb = u'быть'\n",
    "verb_parse = morph.parse(verb)[0]\n",
    "femn_gram = {'VERB','impf','femn','sing','past','indc'}\n",
    "musc_gram = {'VERB','impf','musc','sing','past','indc'}\n",
    "\n",
    "femn_name = [u'Катя',u'Оля',u'Юля']\n",
    "musc_name = [u'Митя',u'Сережа',u'Андрей']\n",
    "\n",
    "\n",
    "name = raw_input(u'Привет, как тебя зовут? ')\n",
    "name = name.decode('utf-8')\n",
    "if name in femn_name:\n",
    "    print u'%s, где ты %s всю прошлую ночь?' %(name,verb_parse.inflect(femn_gram).word)\n",
    "elif name in musc_name:\n",
    "    print u'%s, где ты %s всю прошлую ночь?' %(name,verb_parse.inflect(musc_gram).word)\n",
    "else:\n",
    "    print u'%s, как прошла ночь?' %(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "1. Записать произвольный текст в файл.\n",
    "2. Считать текст из файла.\n",
    "3. Используя оба морфологических процессора, найти все существительные в тексте. На выходе должно быть 2 независимых списка существительных.\n",
    "4. Посчитать количество совпадений и различий.\n",
    "5. Используя nltk.FreqDist(), посчитать частоты всех существительных. На выходе должно быть 2 независимых частотных словаря. \n",
    "6. Найти 10 самых частых существительных в каждом частотном словаре (nltk.FreqDist().most_common()) и вывести их в цикле.\n",
    "7. Можно ли эти существительные считать *ключевыми словами*? \n",
    "\n",
    "## Задание 2\n",
    "1. Написать программу, которая ведет осмысленный диалог и склоняет существительные и глаголы. \n",
    "\n",
    "## Задание 3\n",
    "1. Найти два текста разных стилей (или жанров), например, описание продукта из интернет-магазина и инструкцию по его употреблению.\n",
    "2. Использовать любой морфологический процессор для разбора обоих текстов. На выходе должно быть два разобранных текста: каждый текст представлен списком из списков, каждый мини-список – это пара [слово, POS тег]. \n",
    "3. Посчитать, сколько раз какие части речи употребляются в каждом тексте (можно использовать nltk.FreqDist()).\n",
    "4. Можно ли на основе частот разных частей речи делать выводы о стилевых различиях между текстами?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Литература\n",
    "\n",
    "* Manning & Shuetze: Ch. 10 – Part of Speech Tagging\n",
    "\n",
    "* NLTK Book: Ch. 1, 3.1   Frequency Distributions – частотные словари\n",
    "\n",
    "* Статья Сегаловича про mystem – http://download.yandex.ru/company/iseg-las-vegas.pdf\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "Russian_POS_tagger.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
