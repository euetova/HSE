{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pattern.web import Wikipedia, plaintext\n",
    "import string\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikiParser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def text_cleaning(self, text):\n",
    "        exclude = string.punctuation + '0123456789–«»(?:[]|$)'        \n",
    "        text = ''.join([ch for ch in text if ch not in exclude])\n",
    "        text = re.sub('\\s{2,}',' ', text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "        \n",
    "    def get_articles(self, start, depth=1, max_count=1):\n",
    "        article = Wikipedia().article(start)\n",
    "        links = article.links\n",
    "        list_of_strings = []\n",
    "        for link in links:\n",
    "            text = Wikipedia().article(link)\n",
    "            text = self.text_cleaning(plaintext(text.source))\n",
    "            list_of_strings.append(text)\n",
    "        return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextStatistics:\n",
    "    def __init__(self, articles):\n",
    "        self.articles = articles\n",
    "     \n",
    "    def get_top_3grams(self, n):\n",
    "        all_3grams = []\n",
    "        for a in self.articles:\n",
    "            all_3grams += ngrams(a.split(), 3)\n",
    "        c = Counter(all_3grams)\n",
    "        most_common_n = c.most_common(n)\n",
    "        list_of_3grams_in_descending_order_by_freq = [x[0] for x in most_common_n]\n",
    "        list_of_their_corresponding_freq = [x[1] for x in most_common_n]\n",
    "        return (list_of_3grams_in_descending_order_by_freq, list_of_their_corresponding_freq)\n",
    "    \n",
    "    def get_top_words(self, n):\n",
    "        all_words = []\n",
    "        stop_words = ['a', 'an', 'the', 'as', 'in', 'out', 'on', 'off', 'until', 'of', 'at', 'by', 'for', 'with', \n",
    "                      'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'and', 'but', 'or',\n",
    "                      'to', 'from', 'up', 'down', 'since', 'over', 'under', 'about', 'against', 'like', 'via', 'not']\n",
    "        for a in self.articles:\n",
    "            all_words += [x for x in a.split() if x not in stop_words]\n",
    "        c = Counter(all_words)\n",
    "        most_common_n = c.most_common(n)\n",
    "        list_of_words_in_descending_order_by_freq = [x[0] for x in most_common_n]\n",
    "        list_of_their_corresponding_freq = [x[1] for x in most_common_n]\n",
    "        return (list_of_words_in_descending_order_by_freq, list_of_their_corresponding_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, article):\n",
    "        self.article = article\n",
    "        self.parser = WikiParser()\n",
    "        \n",
    "    def show_results(self):\n",
    "        statistics_links = TextStatistics(self.parser.get_articles(self.article))\n",
    "        top_3grams_links = statistics_links.get_top_3grams(20)\n",
    "        top_words_links = statistics_links.get_top_words(20)\n",
    "        print('For links in article\\nTop 20 3grams:')\n",
    "        print('\\n'.join([' '.join(w)+' : '+str(n) for w, n in zip(top_3grams_links[0], top_3grams_links[1])]))\n",
    "        print('\\nTop 20 words:')\n",
    "        print('\\n'.join([w+' : '+str(n) for w, n in zip(top_words_links[0], top_words_links[1])]))\n",
    "        \n",
    "        statistics = TextStatistics([self.parser.text_cleaning(plaintext(Wikipedia().article(self.article).source))])\n",
    "        top_3grams = statistics.get_top_3grams(5)\n",
    "        top_words = statistics.get_top_words(5)\n",
    "        print('\\nFor article\\nTop 5 3grams:')\n",
    "        print('\\n'.join([' '.join(w)+' : '+str(n) for w, n in zip(top_3grams[0], top_3grams[1])]))\n",
    "        print('\\nTop 5 words:')\n",
    "        print('\\n'.join([w+' : '+str(n) for w, n in zip(top_words[0], top_words[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For links in article\n",
      "Top 20 3grams:\n",
      "natural language processing : 336\n",
      "from the original : 306\n",
      "archived from the : 296\n",
      "v t e : 277\n",
      "the original on : 238\n",
      "the use of : 238\n",
      "as well as : 223\n",
      "one of the : 205\n",
      "a b c : 186\n",
      "proceedings of the : 182\n",
      "the european union : 163\n",
      "cambridge university press : 158\n",
      "of the european : 155\n",
      "such as the : 151\n",
      "the number of : 143\n",
      "a number of : 141\n",
      "university press isbn : 140\n",
      "for example the : 136\n",
      "a set of : 131\n",
      "based on the : 130\n",
      "\n",
      "Top 20 words:\n",
      "is : 8681\n",
      "that : 4821\n",
      "are : 4300\n",
      "language : 4073\n",
      "be : 3397\n",
      "it : 2726\n",
      "this : 2527\n",
      "which : 2204\n",
      "can : 1965\n",
      "english : 1801\n",
      "was : 1796\n",
      "speech : 1739\n",
      "languages : 1709\n",
      "retrieved : 1708\n",
      "such : 1700\n",
      "words : 1666\n",
      "also : 1658\n",
      "have : 1655\n",
      "other : 1629\n",
      "word : 1560\n",
      "\n",
      "For article\n",
      "Top 5 3grams:\n",
      "natural language processing : 16\n",
      "chunk of text : 6\n",
      "a chunk of : 6\n",
      "of natural language : 5\n",
      "proceedings of the : 4\n",
      "\n",
      "Top 5 words:\n",
      "language : 59\n",
      "is : 48\n",
      "natural : 39\n",
      "such : 30\n",
      "processing : 28\n"
     ]
    }
   ],
   "source": [
    "x = Experiment('Natural language processing')\n",
    "x.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
