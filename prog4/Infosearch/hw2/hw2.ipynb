{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Написать краулер, который собирает тексты с новостного ресурса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://susnov.ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_pages(url):\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    links = []\n",
    "    for h in soup.findAll(attrs={'class' : re.compile(\"iceMenuTitle$\")}):\n",
    "        if 'http' not in h['href']:\n",
    "            links.append(url + h['href'])\n",
    "        else:\n",
    "            links.append(h['href'])\n",
    "\n",
    "    # почему-то в меню не все подкатегории, добираем их\n",
    "    extra_links = []\n",
    "    for link in links:\n",
    "        req = requests.get(link)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        for h in soup.findAll(attrs={'class' : \"cat-children\"}):\n",
    "            a = h.findAll('a')\n",
    "            for i in a:\n",
    "                if ':' not in i['href']: # убираем ссылки на другие сайты и прочий мусор\n",
    "                    extra_links.append(url + i['href'])\n",
    "    links = set(links)\n",
    "    links |= set(extra_links)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://susnov.ru/',\n",
       " 'http://susnov.ru/glavnaya/ekonomika.html',\n",
       " 'http://susnov.ru/glavnaya/kultura-turizm-molodjozhnaya-politika.html',\n",
       " 'http://susnov.ru/glavnaya/obrazovanie.html',\n",
       " 'http://susnov.ru/glavnaya/pravoporyadok.html',\n",
       " 'http://susnov.ru/glavnaya/proisshestviya.html',\n",
       " 'http://susnov.ru/glavnaya/sport.html',\n",
       " 'http://susnov.ru/glavnaya/v-rajonakh-oblasti.html',\n",
       " 'http://susnov.ru/glavnaya/vlast-i-obshchestvo.html',\n",
       " 'http://susnov.ru/glavnaya/zdravookhranenie.html',\n",
       " 'http://susnov.ru/k-75-letiyu-pobedy.html',\n",
       " 'http://susnov.ru/k-75-letiyu-pobedy/altar-pobedy.html',\n",
       " 'http://susnov.ru/k-75-letiyu-pobedy/khroniki-pobedy.html',\n",
       " 'http://susnov.ru/k-75-letiyu-pobedy/urok-muzhestva.html',\n",
       " 'http://susnov.ru/o-gazete.html',\n",
       " 'http://susnov.ru/o-gazete/prajs.html',\n",
       " 'http://susnov.ru/rasskazyvaet-obshchestvennyj-korrespondent.html',\n",
       " 'http://susnov.ru/rasskazyvaet-obshchestvennyj-korrespondent/kudryavtsev-valentin-nikiforovich.html',\n",
       " 'http://susnov.ru/rasskazyvaet-obshchestvennyj-korrespondent/tsareva-galina-yurevna.html',\n",
       " 'http://susnov.ru/segodnya-v-gazete.html',\n",
       " 'http://susnov.ru/segodnya-v-gazete/122-chitat-gazetu.html',\n",
       " 'http://susnov.ru/segodnya-v-gazete/123-podpiska.html',\n",
       " 'http://susnov.ru/vashi-prava.html'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main = main_pages(url)\n",
    "main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# собираем ссылки на статьи с данной страницы \n",
    "def take_links(url, soup):\n",
    "    links = []\n",
    "    for h in soup.findAll(attrs={'class' : 'blog blog'}):\n",
    "        a = h.findAll('a', title = False)\n",
    "        for i in a:\n",
    "            if ':' not in i['href']: \n",
    "                links.append(url + i['href'])\n",
    "            \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_article_links(url):\n",
    "    links = []\n",
    "    main = main_pages(url)\n",
    "    \n",
    "    for link in main:\n",
    "        req = requests.get(link)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        links += take_links(url, soup)\n",
    "        try:\n",
    "            pages = int(soup.find('a', {'title':'В конец'})['href'].split('=')[1])\n",
    "            for n in range(7, pages, 7):\n",
    "                req = requests.get(link+'?start='+str(n))\n",
    "                soup = BeautifulSoup(req.text, 'lxml')\n",
    "                links += take_links(url, soup)\n",
    "        except:\n",
    "            pass\n",
    "    links = set(links)\n",
    "    links -= set(main)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_articles(url):\n",
    "    article_links = all_article_links(url)\n",
    "    print('Всего статей: ', len(article_links))\n",
    "    for link in article_links:\n",
    "        req = requests.get(link)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        author = soup.find('strong').text.split('. ')[0]\n",
    "        title = soup.find('h2').text.strip()\n",
    "        date = soup.find('dd', attrs = {'class': 'published'}).text.split()[1]\n",
    "        # есть статьи с неуказанной категорией\n",
    "        try:\n",
    "            topic = soup.find('dd', attrs = {'class': 'category-name'}).find('a').text\n",
    "        except:\n",
    "            topic = 'None'\n",
    "        # иногда текст статьи находится вне тегов <p></p>\n",
    "        try:\n",
    "            text = soup.find('div', attrs = {'class':'item-page item-page'}).findAll('p')\n",
    "            text = ' '.join([t.text for t in text])\n",
    "        except AttributeError:\n",
    "            text = soup.find('div', attrs = {'class':'item-page item-page'}).text\n",
    "        # а тут ругается на длинные названия статей == txt файлов\n",
    "        try:\n",
    "            with open(title+'.txt', 'w') as file:\n",
    "                file.write('@au '+author+'\\n@ti '+title+'\\n@da '+date+'\\n@topic '+topic+'\\n@url '+link+'\\n'+text)\n",
    "        except OSError:\n",
    "            title = title[:40]\n",
    "            with open(title+'.txt', 'w') as file:\n",
    "                file.write('@au '+author+'\\n@ti '+title+'\\n@da '+date+'\\n@topic '+topic+'\\n@url '+link+'\\n'+text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего статей:  4977\n"
     ]
    }
   ],
   "source": [
    "write_articles(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
