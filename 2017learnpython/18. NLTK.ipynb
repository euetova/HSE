{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) - набор модулей для работы с естественным языком, включающий библиотеки для работы с корпусами, статистической обработки, токенизации, стемминга, парсинга и т.д.  http://www.nltk.org/\n",
    "\n",
    "Подробный туториал по работе с NLTK - http://www.nltk.org/book/ (но он скорее для новичков или очень неуверенных пользователей).\n",
    "\n",
    "\n",
    "## Токенизация\n",
    "\n",
    "HOWTO - http://www.nltk.org/howto/tokenize.html\n",
    "\n",
    "NLTK позволяет использовать токенизаторы по умолчанию для всех языков. Обратите внимание, что знаки препинания тоже считаются токенами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Библиотека', 'NLTK', ',', 'или', 'NLTK', '—', 'пакет', 'библиотек', 'и', 'программ', 'для', 'символьной', 'и', 'статистической', 'обработки', 'естественного', 'языка', ',', 'написанных', 'на', 'языке', 'программирования', 'Python', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "rus_text = 'Библиотека NLTK, или NLTK — пакет библиотек и программ для символьной и статистической обработки естественного языка, написанных на языке программирования Python.'\n",
    "tokens = word_tokenize(rus_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', '(', 'Natural', 'Language', 'Toolkit', ')', '-', 'набор', 'модулей', 'для', 'работы', 'с', 'естественным', 'языком', ',', 'включающий', 'библиотеки', 'для', 'работы', 'с', 'корпусами', ',', 'статистической', 'обработки', ',', 'токенизации', ',', 'стемминга', ',', 'парсинга', 'и', 'т.д', '.']\n"
     ]
    }
   ],
   "source": [
    "rus_text = 'NLTK (Natural Language Toolkit) - набор модулей для работы с естественным языком, включающий библиотеки для работы с корпусами, статистической обработки, токенизации, стемминга, парсинга и т.д.'\n",
    "tokens = word_tokenize(rus_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на то, как токенизируются числа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366.88', '.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
    "print(word_tokenize(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сокращения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'called', 'Dr.', 'Jones', '.', 'I', 'called', 'Dr.', 'Jones', '.']\n"
     ]
    }
   ],
   "source": [
    "s11 = \"I called Dr. Jones. I called Dr. Jones.\"\n",
    "print(word_tokenize(s11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того NLTK позволяет создать собственный токенизатор с помощью регулярных выражений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции `regexp_tokenize` нужно передать текст для токенизации, регулярное выражение и указать, как именно это выражение использовать:\n",
    "* `gaps=False` - вернет массив с фрагментами текста, которые совпали с регулярным выражением,\n",
    "* `gaps=True` - вернет массив с фрагментами текста, которые не совпали с регулярным выражением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<p>', '<b>', '</b>', '</p>']\n"
     ]
    }
   ],
   "source": [
    "s = \"<p>Although this is <b>not</b> the case here, we must not relax our vigilance!</p>\"\n",
    "print(regexp_tokenize(s, r'</?[bp]>', gaps=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although this is ', 'not', ' the case here, we must not relax our vigilance!']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(s, r'</?[bp]>', gaps=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в выражении есть группы в скобках, то в итоговый массив войдут фрагменты, которые соответствуют группам в скобках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'b', 'b', 'p']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(s, r'</?(b|p)>', gaps=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'Although this is ', 'b', 'not', 'b', ' the case here, we must not relax our vigilance!', 'p']\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(s, r'</?(b|p)>', gaps=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ough', 'this'), ('the', 'case'), ('we', 'must'), ('not', 'rela'), ('our', 'vigi')]\n"
     ]
    }
   ],
   "source": [
    "print(regexp_tokenize(s, '(\\w{2,4}) (\\w{2,4})', gaps=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярные выражения с обратными ссылками (backreferences) вызывают ошибку!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные простые функции\n",
    "\n",
    "`print_string` расставляет переносы так, чтобы длина печатной строки не привысила определенного значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a long string,\n",
      "therefore it should break\n"
     ]
    }
   ],
   "source": [
    "nltk.print_string(\"This is a long string, therefore it should break\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK (Natural Language Toolkit) - набор модулей\n",
      "для работы с естественным языком, включающий\n",
      "библиотеки для работы с корпусами, статистической\n",
      "обработки, токенизации, стемминга, парсинга и т.д.\n"
     ]
    }
   ],
   "source": [
    "nltk.print_string(\"NLTK (Natural Language Toolkit) - набор модулей для работы с естественным языком, включающий библиотеки для работы с корпусами, статистической обработки, токенизации, стемминга, парсинга и т.д.\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re_show` позволяет посмотреть, какая часть строки совпадает с регулярным выражением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{sdf}123\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(\"[a-z]+\", \"sdf123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`edit_distance` позволяет посчитать расстояние редактирования между словами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import *\n",
    "edit_distance(\"муха\", \"слон\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стеммер \n",
    "\n",
    "HOWTO  - http://www.nltk.org/howto/stem.html\n",
    "\n",
    "Стеммер позволяет отрезать от слов окончания и выделять основы. В NLTK доступны стеммеры для нескольких языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на русский стеммер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "бегущ\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")\n",
    "print(stemmer.stem(\"бегущий\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK ( Natura Languag Toolk ) - набор модул для работ с естествен язык , включа библиотек для работ с корпус , статистическ обработк , токенизац , стемминг , парсинг и т.д .\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(rus_text)\n",
    "stems = [stemmer.stem(t) for t in tokens]\n",
    "print(' '.join(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Корпуса\n",
    "\n",
    "Внутри NLTK находится большое количество готовых корпусов (размеченный Брауновский корпус, тексты из проекта Гутенберг, интернет-тексты и чаты, корпус президентских речей, ворднет, трибанк, переводы декларации прав человека на более чем 300 языков и т.д.) Ко всем корпусам можно обратиться из питона через уже готовые объекты класса CorpusReader. Про них можно почитать в главе 2 учебника по NLTK, она посвященна этим корпусам - http://www.nltk.org/book/ch02.html\n",
    "\n",
    "Кроме того, можно воспользоваться возможностями NLTK, чтобы очень быстро загрузить в питон свой собственный корпус.\n",
    "\n",
    "HOWTO - http://www.nltk.org/howto/corpus.html\n",
    "\n",
    "Например, в папке `texts` у меня находятся тексты студентов, изучающих русский язык, каждый текст в отдельном текстовом файле, никакой разметки там нет. Все эти тексты можно загрузить в питон одной строчкой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_corpus = nltk.corpus.PlaintextCorpusReader('./texts', '.*\\.txt')\n",
    "# первый аргумент - корень - путь к папке с корпусом\n",
    "# второй аргумент - либо регулярное выражение, описывающее имена файлов, либо массив с именами файлов, которые нужно прочитать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого можно получить все предложения с помощью метода `sents`,  результатом является массив массивов, каждый массив - предложение = массив токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Загрязнение', 'тяжелыми', 'металлами', 'Дальнегорского', 'района', '.'], ['Одной', 'из', 'самых', 'главных', 'экологических', 'проблем', 'на', 'территории', 'Российской', 'Федерации', 'является', 'загрязнение', 'окружющей', 'среды', 'тяжелыми', 'металлами', ',', 'такими', 'как', 'свинeц', ',', 'кадмий', 'и', 'цинк', '.'], ['Эта', 'проблема', 'особеннo', 'характерна', 'для', 'тех', 'местностей', ',', 'где', 'добывается', 'руда', 'и', 'выплавляется', 'свинец', '-', 'Дальнегорский', 'район', 'Приморского', 'края', ',', 'долина', 'реки', 'Рудной', ',', 'пос', '.'], ['Рудная', 'Пристань', '.'], ['Согласно', 'проведенным', 'исследованиям', 'Тихоокеанского', 'института', 'географии', ',', 'по', 'уровню', 'загрязнения', 'почв', 'пос', '.']]\n"
     ]
    }
   ],
   "source": [
    "# .sents() - массив предложений\n",
    "print(my_corpus.sents()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('C:\\\\Users\\\\Admin\\\\PycharmProjects\\\\2017learnpython\\\\texts')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# путь к корню\n",
    "my_corpus.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt', '10.txt', '100.txt', '1000.txt', '1001.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# массив с именами всех файлов в корпусе\n",
    "my_corpus.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Загрязнение', 'тяжелыми', 'металлами', 'Дальнегорского', 'района', '.', 'Одной', 'из', 'самых', 'главных']\n"
     ]
    }
   ],
   "source": [
    "# массив всех токенов в корпусе\n",
    "print(my_corpus.words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881030"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_corpus.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Загрязнение тяжелыми металлами Дальнего'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чистый текст корпуса (из нескольких файлов объединенный в одну строку)\n",
    "my_corpus.raw()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[437, 95, 118, 82, 454, 128, 253, 305, 304, 239]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считаем, сколько слов в первых 10 файлах\n",
    "# .words() может принимать на вход имя файла\n",
    "[len(my_corpus.words(d)) for d in my_corpus.fileids()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1048.txt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text50 = my_corpus.fileids()[50]\n",
    "text50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Участие', 'в', 'программе', 'мне', 'дало', 'способность', 'говорить', ',', 'читать', 'и', 'писать', 'подробно', 'о', 'разнообраных', 'темах', 'жизни', ',', ',', 'политики', 'и', 'так', 'далее', '.', 'Я', 'более', 'сомоувереный', 'в', 'моём', 'способноти', 'говорить', 'по', '-', 'русский', '.', '.', 'Я', 'смал', 'больше', 'говорить', 'и', 'обуждать', 'по', '-', 'русски', 'с', 'русскими', ',', 'с', 'которуми', 'я']\n"
     ]
    }
   ],
   "source": [
    "print(my_corpus.words(text50)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коллокации\n",
    "\n",
    "HOWTO - http://www.nltk.org/howto/collocations.html \n",
    "\n",
    "Коллокации - это выражения из нескольких слов, которые часто встречаются вместе. Для выделения коллокаций существуют специальные метрики - функции для всех этих метрик доступны в NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(my_corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутри `from_words` можно также указать параметр `window_size`, например `window_size=4`, тогда в массив биграмм попадут не только пары слов, которые находятся рядом, но и все пары слов, которые находятся на расстоянии не более 4.\n",
    "\n",
    "Пример:\n",
    "     finder = TrigramCollocationFinder.from_words(tokens, window_size=4)\n",
    "     \n",
    "Посмотрим, какие биграммы нашлись:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!»).', 'Старшие'), ('(\"', 'Событие'), ('(‘', 'Мной'), ('++,', 'Basic'), ('-\"-', 'еМ'), ('.&#', '61472'), ('.....»', 'Предложение'), ('1043', 'штатных'), ('120000', 'куб'), ('15000', 'микрорентген')]\n"
     ]
    }
   ],
   "source": [
    "print( finder.nbest(bigram_measures.pmi, 10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы улучшить результат, можно игнорировать биграммы, которые встретились меньше 3 раз:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':/', 'Могучей'), ('Home', 'visits'), ('Notre', 'Dame'), ('internet', 'resources'), ('iron', 'curtain'), ('modus', 'vivendi'), ('Абаб', 'Тамыр'), ('Альберт', 'Гор'), ('Вотще', 'рвалась'), ('Вульф', 'собирался')]\n"
     ]
    }
   ],
   "source": [
    "finder.apply_freq_filter(3)\n",
    "print( finder.nbest(bigram_measures.pmi, 10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нас интересуют только определенные слова (например, только русские), можно добавить фильтр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Абаб', 'Тамыр'), ('Альберт', 'Гор'), ('Вотще', 'рвалась'), ('Вульф', 'собирался'), ('Вьетнамцы', 'заключали'), ('Гаагской', 'конвенции'), ('Гарольда', 'Байрона'), ('Герасимович', 'Зыбелин'), ('Кавказская', 'пленница'), ('Лилей', 'Брик')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# удалим из анализа слова короче 3х букв и не содержащие кириллических символов\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or re.search('[а-яё]+', w.lower()) is None)\n",
    "print( finder.nbest(bigram_measures.pmi, 10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примерах выше мы распечатывали 10 лучших биграмм по метрике PMI (pointwise mutual information). Посмотрим на другие метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('потому', 'что'), ('самом', 'деле'), ('может', 'быть'), ('Что', 'такое'), ('Таким', 'образом'), ('точки', 'зрения'), ('Кроме', 'того'), ('русского', 'языка'), ('Советского', 'Союза'), ('сих', 'пор')]\n"
     ]
    }
   ],
   "source": [
    "print( finder.nbest(bigram_measures.likelihood_ratio, 10) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('потому', 'что'), ('что', 'они'), ('может', 'быть'), ('что', 'это'), ('Что', 'такое'), ('так', 'как'), ('что', 'она'), ('самом', 'деле'), ('Кроме', 'того'), ('Таким', 'образом')]\n"
     ]
    }
   ],
   "source": [
    "print( finder.nbest(bigram_measures.raw_freq, 10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также посмотреть и на сами числа, соответствующие коллокациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('потому', 'что'), 8498.478921826874), (('самом', 'деле'), 3979.2590675549236), (('может', 'быть'), 3608.282467673831), (('Что', 'такое'), 3430.6486589703927), (('Таким', 'образом'), 3241.9499515941197), (('точки', 'зрения'), 2842.209363718584), (('Кроме', 'того'), 2455.2140000114678), (('русского', 'языка'), 2246.202511548758), (('Советского', 'Союза'), 2236.4322570395493), (('сих', 'пор'), 2121.5946044332572)]\n"
     ]
    }
   ],
   "source": [
    "scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "print(scored[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще можно посмотреть на частоты всех биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('чтобы', 'выражать') 4\n",
      "('международная', 'организация') 6\n",
      "('юношеского', 'питания') 9\n",
      "('качеств', 'является') 3\n",
      "('социальное', 'развитие') 3\n",
      "('мнению', 'авторов') 8\n",
      "('современном', 'языке') 3\n",
      "('буду', 'говорить') 15\n",
      "('нарушение', 'прав') 3\n",
      "('моим', 'другом') 6\n"
     ]
    }
   ],
   "source": [
    "freqs = finder.ngram_fd\n",
    "for key, value in list(freqs.items())[:10]:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительно\n",
    "\n",
    "Список всех модулей внутри NLTK - http://www.nltk.org/howto/. Там например можно найти модели для статистических подсчетов, создания марковских моделей, морфологического разбора с использованием готовых таггеров или обучения новых таггеров, построения синтаксических деревьев, работы с твиттером, анализа эмоциональной окраски и многое другое.\n",
    "\n",
    "Репозиторий проекта на гитхабе - https://github.com/nltk/nltk/wiki. Там в частности можно посмотреть часто задаваемые вопросы, а еще узнать, какие фичи сейчас в разработке. Проект опенсорсный, так что можно помочь сообществу, добавив туда какой-то свой код.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание \n",
    "\n",
    "В ваш сайт на фласке нужно добавить еще одно приложение с использованием хотя бы одного из перечисленного: Pymorphy2, Cognitive Services, NLTK.\n",
    "\n",
    "Примеры:\n",
    "\n",
    "1) Задание из конспекта про Pymorphy2. \n",
    "\n",
    "  * Написать программу-бота, с которой можно разговаривать на вашем сайте: пользователь пишет ей реплику, а она отвечает предложением, в котором все слова заменены на какие-то случайные другие слова той же части речи и с теми же грамматическими характеристиками. Предложение-ответ должно быть согласованным. Например, на фразу \"Мама мыла раму\" программа может ответить \"Девочка пела песню\".\n",
    "\n",
    "2) Вариант задания по NLTK.\n",
    "\n",
    "  * Собрать небольшой корпус (например, корпус субтитров к сериалам или корпус песен вашей любимой группы) и сделать страницу с поиском по этому корпусу. На этой странице должна быть возможность искать предложения по точной форме слова, по основе, желательно еще и по части речи (за бонус), в также возможность выводить несколько самых частотных коллокаций или все коллокации с заданным словом. \n",
    "   \n",
    "3) Задание про Когнитивные Сервисы.\n",
    "\n",
    "  * Напишите страницу, на которой можно загрузить изображение с текстом и получить в ответ текст с картинки. Бонус можно получить, если изображение может содержать как печатный, так и рукописный текст.\n",
    "   \n",
    "4) Можно придумать своё приложение.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
