{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wikitexts(): # скачиваем по 100 статей для каждого языка. Это может занять какое-то время (5-10 минут. как правило)\n",
    "    wiki_texts = {}\n",
    "    langs = ['es', 'uk', 'be', 'fr']\n",
    "    for lang in langs: # испанский в википедии — это es, украинский — uk, а белорусский — be\n",
    "        wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "        print(lang, len(wiki_texts[lang]))\n",
    "    return wiki_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file E:\\programs\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page La Aldea (desambiguación)\n",
      "Skipping page YAT\n",
      "Skipping page Maquilla\n",
      "Skipping page Rose Hill (desambiguación)\n",
      "Skipping page Diamond Lake\n",
      "es 95\n",
      "Skipping page Єлизаветіно\n",
      "Skipping page 1482 (значення)\n",
      "Skipping page Максютово (Стерлітамацький район)\n",
      "Skipping page Аккурган\n",
      "Skipping page Церква Різдва Христового\n",
      "Skipping page Гутиря\n",
      "Skipping page Сент-Обен\n",
      "Skipping page Кассандра (значення)\n",
      "Skipping page Вулиця Дворцова\n",
      "Skipping page Сайджьо\n",
      "Skipping page Йоффе\n",
      "Skipping page Лопушне (Міжгірський район)\n",
      "uk 88\n",
      "Skipping page Сцяпанаўка\n",
      "Skipping page Новік\n",
      "Skipping page Уласавічы\n",
      "Skipping page Сяркі\n",
      "Skipping page Чэмпіянат Беларусі па грэка-рымскай барацьбе 2015\n",
      "Skipping page Дрычын\n",
      "Skipping page Вярхоўскі\n",
      "Skipping page Вялікі Бор\n",
      "Skipping page Арэналь\n",
      "Skipping page Баханскі сельсавет\n",
      "Skipping page Шарсні\n",
      "be 89\n",
      "Skipping page Schindler\n",
      "Skipping page Le Premier Pas\n",
      "Skipping page Lehane\n",
      "Skipping page Les Casseurs\n",
      "Skipping page Brody\n",
      "fr 95\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = wikitexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens(file):  # находит токены\n",
    "    exclude = set(punctuation + '0123456789'+'–—'+'«»')\n",
    "    file = ''.join(ch for ch in file if ch not in exclude)\n",
    "    tkns = WhitespaceTokenizer().tokenize(file.lower()) \n",
    "    return tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freqdict(corpus):\n",
    "    freqwords = []\n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    \n",
    "    for article in corpus:\n",
    "        article = tokens(article)\n",
    "        for word in article:\n",
    "            if '' != word:\n",
    "                freqs[word] += 1\n",
    "        \n",
    "    for word in sorted(freqs, key=lambda w: freqs[w], reverse=True)[:50]:\n",
    "        #print('{}\\t{}'.format(freqs[word], word))\n",
    "        freqwords.append(word)\n",
    "    \n",
    "    return freqwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_r = set(freqdict(wiki_texts['es']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_r = set(freqdict(wiki_texts['fr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uk_r = set(freqdict(wiki_texts['uk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "be_r = set(freqdict(wiki_texts['be']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# множество из всех элементов, не принадлежащие другим\n",
    "es = es_r.difference(fr_r, uk_r, be_r)\n",
    "uk = uk_r.difference(fr_r, es_r, be_r)\n",
    "be = be_r.difference(fr_r, uk_r, es_r)\n",
    "fr = fr_r.difference(es_r, uk_r, be_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probab(file, freqd):\n",
    "    k = 0\n",
    "    for word in file:\n",
    "        if word in freqd:\n",
    "            k += 1\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def langdetect(file, es, uk, be, fr):\n",
    "    c_es = probab(file, es)\n",
    "    c_uk = probab(file, uk)\n",
    "    c_be = probab(file, be)\n",
    "    c_fr = probab(file, fr)\n",
    "    c_all = c_fr + c_es + c_uk + c_be\n",
    "    p_es = c_es/c_all\n",
    "    p_uk = c_uk/c_all\n",
    "    p_be = c_be/c_all\n",
    "    p_fr = c_fr/c_all\n",
    "    probability = max(p_es, p_uk, p_be, p_fr)\n",
    "    if probability == p_es:\n",
    "        return 'es'\n",
    "    elif probability == p_uk:\n",
    "        return 'uk'\n",
    "    elif probability == p_be:\n",
    "        return 'be'\n",
    "    else:\n",
    "        return 'fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uk'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langdetect(tokens(wiki_texts['uk'][4]), es, uk, be, fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_dict(lang):\n",
    "    true = []\n",
    "    pred = []\n",
    "    for i in range(len(wiki_texts[lang])):\n",
    "        true.append(lang)\n",
    "        pred.append(langdetect(tokens(wiki_texts[lang][i]), es, uk, be, fr))\n",
    "    return accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98863636363636365"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_ngr(corpus):\n",
    "    freqs_ngr = collections.defaultdict(lambda: 0)\n",
    "    freqngrms = []\n",
    "\n",
    "    for article in corpus:\n",
    "        for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "            freqs_ngr[ngram] += 1\n",
    "\n",
    "    for ngram in sorted(freqs_ngr, key=lambda n: freqs_ngr[n], reverse=True)[:50]:\n",
    "       # print('{}\\t{}'.format(freqs_ngr[ngram], ngram))\n",
    "        freqngrms.append(ngram)\n",
    "    \n",
    "    return freqngrms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_rn = set(freq_ngr(wiki_texts['es']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uk_rn = set(freq_ngr(wiki_texts['uk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "be_rn = set(freq_ngr(wiki_texts['be']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_rn = set(freq_ngr(wiki_texts['fr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_n = es_rn.difference(fr_rn, uk_rn, be_rn)\n",
    "uk_n = uk_rn.difference(fr_rn, es_rn, be_rn)\n",
    "be_n = be_rn.difference(fr_rn, uk_rn, es_rn)\n",
    "fr_n = fr_rn.difference(es_rn, uk_rn, be_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_ngrms(lang):\n",
    "    true = []\n",
    "    pred = []\n",
    "    for i in range(len(wiki_texts[lang])):\n",
    "        true.append(lang)\n",
    "        pred.append(langdetect(make_ngrams(wiki_texts[lang][i]), es_n, uk_n, be_n, fr_n))\n",
    "    return accuracy_score(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98947368421052628"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('be')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом методе результат для украинского был хуже, чем у других. Во втором методе для испанского. Но разница не большая. Скорее всего дело в текстах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file E:\\programs\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Codificación\n",
      "Skipping page Condado de Cleburne\n",
      "Skipping page Avia\n",
      "Skipping page Mount Pleasant (desambiguación)\n",
      "Skipping page Bloque\n",
      "Skipping page Yubo\n",
      "es 94\n",
      "Skipping page Побережне\n",
      "Skipping page Ляхово (Кармаскалинський район)\n",
      "Skipping page Келдерару\n",
      "Skipping page Перейма\n",
      "Skipping page Клименко Володимир Петрович\n",
      "Skipping page Анан\n",
      "Skipping page Вересоч\n",
      "Skipping page Жебровський\n",
      "Skipping page Балаклійка\n",
      "Skipping page Хмарівка\n",
      "uk 90\n",
      "Skipping page Задворанцы\n",
      "Skipping page Дарашкевіч\n",
      "Skipping page Лужок\n",
      "Skipping page Раман Навумавіч Мачульскі\n",
      "Skipping page Крыштаф Зяновіч\n",
      "be 95\n",
      "Skipping page La Mort de Lucrèce\n",
      "Skipping page Brocco\n",
      "Skipping page Henry Larsen\n",
      "Skipping page Expresso (homonymie)\n",
      "Skipping page Leptothrix\n",
      "Skipping page Cremonini\n",
      "Skipping page Lac qui parle (homonymie)\n",
      "Skipping page Traité de Saint-Maur\n",
      "Skipping page Favier\n",
      "fr 91\n"
     ]
    }
   ],
   "source": [
    "# тестируем\n",
    "wiki_texts_test = wikitexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_r = set(freqdict(wiki_texts_test['es']))\n",
    "fr_r = set(freqdict(wiki_texts_test['fr']))\n",
    "uk_r = set(freqdict(wiki_texts_test['uk']))\n",
    "be_r = set(freqdict(wiki_texts_test['be']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = es_r.difference(fr_r, uk_r, be_r)\n",
    "fr = fr_r.difference(es_r, uk_r, be_r)\n",
    "uk = uk_r.difference(be_r, fr_r, es_r)\n",
    "be = be_r.difference(fr_r, uk_r, es_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98863636363636365"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict('be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "be_rn = set(freq_ngr(wiki_texts_test['be']))\n",
    "uk_rn = set(freq_ngr(wiki_texts_test['uk']))\n",
    "fr_rn = set(freq_ngr(wiki_texts_test['fr']))\n",
    "es_rn = set(freq_ngr(wiki_texts_test['es']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_n = es_rn.difference(be_rn, fr_rn, be_rn)\n",
    "be_n = be_rn.difference(uk_rn, fr_rn, es_rn)\n",
    "uk_n = uk_rn.difference(be_rn, fr_rn, es_rn)\n",
    "fr_n = fr_rn.difference(uk_rn, be_rn, es_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98947368421052628"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9887640449438202"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ngrms('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отчет:\n",
    "    -И в тренировочной выборке и в тестовой для украинского языка результат словарного метода был хуже, чем для остальных языков.\n",
    "    -Также и в тестовой и в тренировочной выборках для испанского языка результат n-gram был хуже, чем для остальных. Распознавание белорусского языка с помощью n-gram в тестовой выборке оказалось хуже, чем в тренировочной "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
